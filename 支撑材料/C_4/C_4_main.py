# å¯¼å…¥å¿…è¦çš„åº“
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import os
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from imblearn.over_sampling import SMOTE
import warnings

warnings.filterwarnings('ignore')

# æ£€æŸ¥ imbalanced-learn åº“æ˜¯å¦å®‰è£…
try:
    import imblearn
    print(f"âœ“ imbalanced-learn ç‰ˆæœ¬: {imblearn.__version__}")
except ImportError:
    print("âœ— æœªæ‰¾åˆ° imbalanced-learn åº“ã€‚")
    print("è¯·è¿è¡Œ: pip install imbalanced-learn")
    raise

print("\nåº“å¯¼å…¥å®Œæˆ")

def setup_chinese_font():
    """è®¾ç½®ä¸­æ–‡å­—ä½“"""
    font_path = 'E:/Mathematics_Modeling_study/2025_CUMCM/fonts/HPSIMPLIFIEDHANS-REGULAR.TTF'
    if os.path.exists(font_path):
        fm.fontManager.addfont(font_path)
        font_prop = fm.FontProperties(fname=font_path)
        plt.rcParams['font.family'] = font_prop.get_name()
        plt.rcParams['axes.unicode_minus'] = False
        print(f"âœ“ æˆåŠŸåŠ è½½å¹¶è®¾ç½®å­—ä½“: {font_prop.get_name()}")
        return font_prop
    else:
        for font_name in ['Microsoft YaHei', 'SimHei', 'PingFang SC']:
            if any(f.name == font_name for f in fm.fontManager.ttflist):
                plt.rcParams['font.family'] = font_name
                plt.rcParams['axes.unicode_minus'] = False
                print(f"âœ“ ä½¿ç”¨ç³»ç»Ÿå­—ä½“: {font_name}")
                return fm.FontProperties(family=font_name)
    raise RuntimeError("æœªèƒ½æ‰¾åˆ°å¯ç”¨çš„ä¸­æ–‡å­—ä½“")

try:
    plt.style.use('seaborn-v0_8-whitegrid')
    sns.set_style("whitegrid")
    font_prop = setup_chinese_font()
    print("âœ“ å­—ä½“è®¾ç½®æˆåŠŸ")
except Exception as e:
    print(f"âœ— å­—ä½“è®¾ç½®å¤±è´¥: {str(e)}")
    raise
# åŠ è½½å¹¶è¿›è¡Œä¸åŸ notebook ç›¸åŒçš„é¢„å¤„ç†
try:
    data_path = '../../Stem/Cé¢˜/é™„ä»¶.xlsx'
    df_female = pd.read_excel(data_path, sheet_name=1)
    print(f"æ•°æ®åŠ è½½æˆåŠŸï¼å¥³èƒæ•°æ®å½¢çŠ¶: {df_female.shape}")
except Exception as e:
    print(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
    # å¤‡ç”¨è·¯å¾„
    try:
        data_path = '../Stem/Cé¢˜/é™„ä»¶.xlsx'
        df_female = pd.read_excel(data_path, sheet_name=1)
        print(f"å¤‡ç”¨è·¯å¾„åŠ è½½æˆåŠŸï¼å¥³èƒæ•°æ®å½¢çŠ¶: {df_female.shape}")
    except Exception as e2:
        print(f"å¤‡ç”¨è·¯å¾„ä¹Ÿå¤±è´¥: {e2}")
        raise

# åˆ›å»ºå·¥ä½œå‰¯æœ¬
df = df_female.copy()

# --- ä¸åŸ notebook ç›¸åŒçš„é¢„å¤„ç†æµç¨‹ ---

# 1. ç›®æ ‡å˜é‡åˆ›å»º
df['Is_Abnormal'] = df['æŸ“è‰²ä½“çš„éæ•´å€ä½“'].notna().astype(int)
df['Detailed_Abnormality'] = df['æŸ“è‰²ä½“çš„éæ•´å€ä½“'].fillna('Normal')

# 2. ç¼ºå¤±å€¼å¤„ç†
# å¯¹äºæ•°å€¼å‹ç‰¹å¾ï¼Œä½¿ç”¨ä¸­ä½æ•°å¡«å……
numeric_columns = df.select_dtypes(include=[np.number]).columns
for col in numeric_columns:
    if df[col].isnull().sum() > 0:
        median_val = df[col].median()
        df[col].fillna(median_val, inplace=True)

# 3. ç‰¹å¾å·¥ç¨‹
z_value_columns = ['13å·æŸ“è‰²ä½“çš„Zå€¼', '18å·æŸ“è‰²ä½“çš„Zå€¼', '21å·æŸ“è‰²ä½“çš„Zå€¼', 'XæŸ“è‰²ä½“çš„Zå€¼']
for col in z_value_columns:
    df[f'abs_{col}'] = df[col].abs()
for col in z_value_columns:
    df[f'{col}_squared'] = df[col] ** 2
abs_z_columns = [f'abs_{col}' for col in z_value_columns]
df['Max_Abs_Z'] = df[abs_z_columns].max(axis=1)
df['Z_above_3_count'] = (df[abs_z_columns] > 3.0).sum(axis=1)
df['Z_mean'] = df[z_value_columns].mean(axis=1)
df['Z_std'] = df[z_value_columns].std(axis=1)

# 4. å‡†å¤‡å»ºæ¨¡ç‰¹å¾
feature_columns = z_value_columns.copy()
derived_z_features = [f'abs_{col}' for col in z_value_columns] + \
                     [f'{col}_squared' for col in z_value_columns] + \
                     ['Max_Abs_Z', 'Z_above_3_count', 'Z_mean', 'Z_std']
feature_columns.extend(derived_z_features)

other_features = ['å­•å¦‡BMI', 'å¹´é¾„'] # ä½¿ç”¨'å­•å¦‡BMI',æ›¿æ¢'å­•å¦‡ BMIæŒ‡æ ‡'
for feat in other_features:
    if feat in df.columns:
        feature_columns.append(feat)

gc_features = ['13å·æŸ“è‰²ä½“çš„GCå«é‡', '18å·æŸ“è‰²ä½“çš„GCå«é‡', '21å·æŸ“è‰²ä½“çš„GCå«é‡']
feature_columns.extend(gc_features)

# ç¡®ä¿åˆ—åå”¯ä¸€ä¸”æœ‰æ•ˆ
feature_columns = list(dict.fromkeys(feature_columns)) # ç§»é™¤é‡å¤é¡¹
valid_features = [f for f in feature_columns if f in df.columns and df[f].dtype in ['int64', 'float64']]

print(f"é¢„å¤„ç†å®Œæˆã€‚ä½¿ç”¨äº† {len(valid_features)} ä¸ªæœ‰æ•ˆç‰¹å¾ã€‚")
print("\næ•°æ®é›†ä¸­çš„å¼‚å¸¸ç±»åˆ«åˆ†å¸ƒ:")
print(df['Detailed_Abnormality'].value_counts())
# å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡
X = df[valid_features].copy()
# æˆ‘ä»¬çš„ç›®æ ‡æ˜¯äºŒåˆ†ç±»ï¼š0 ä»£è¡¨ 'Normal', 1 ä»£è¡¨ 'Any_Abnormal'
# 'Is_Abnormal' åˆ—å·²ç»å®Œç¾åœ°æ»¡è¶³äº†è¿™ä¸ªéœ€æ±‚
y_binary = df['Is_Abnormal'].copy()

# æ ‡ç­¾åç§°ï¼Œç”¨äºåç»­ç»“æœè§£é‡Š
label_names = {0: 'Normal', 1: 'Any_Abnormal'}
print("æ ‡ç­¾æ˜ å°„:")
print(label_names)

# æ‹†åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆå¿…é¡»åœ¨SMOTEä¹‹å‰è¿›è¡Œï¼ï¼‰
# ä½¿ç”¨ stratify ç¡®ä¿è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­çš„å¼‚å¸¸æ¯”ä¾‹ä¸åŸå§‹æ•°æ®ä¸€è‡´
X_train, X_test, y_train, y_test = train_test_split(
    X, y_binary, 
    test_size=0.3, 
    random_state=42, 
    stratify=y_binary
)

# ç‰¹å¾æ ‡å‡†åŒ–
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"\nåŸå§‹è®­ç»ƒé›†å½¢çŠ¶: {X_train_scaled.shape}")
print("åŸå§‹è®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒ:")
print(y_train.value_counts())

# åº”ç”¨SMOTEè¿›è¡Œæ•°æ®å¢å¼º
# SMOTEåªä¼šå¯¹è®­ç»ƒæ•°æ®è¿›è¡Œè¿‡é‡‡æ ·ï¼Œä»¥é¿å…æ•°æ®æ³„éœ²
# å¯¹äºäºŒåˆ†ç±»é—®é¢˜ï¼ŒSMOTEå°†è‡ªåŠ¨å¢åŠ å°‘æ•°ç±»ï¼ˆå¼‚å¸¸æ ·æœ¬ï¼‰çš„æ•°é‡
print(f"\nåº”ç”¨SMOTEå¤„ç†ä¸å¹³è¡¡...")
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)

print(f"\nSMOTEå¢å¼ºåçš„è®­ç»ƒé›†å½¢çŠ¶: {X_train_smote.shape}")
print("SMOTEå¢å¼ºåçš„è®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒ:")
print(y_train_smote.value_counts())

# åˆå§‹åŒ–å¹¶è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹
# class_weight='balanced' ä¼šè‡ªåŠ¨ä¸ºæ ·æœ¬é‡å°‘çš„ç±»åˆ«èµ‹äºˆæ›´é«˜çš„æƒé‡
# å³ä½¿ä½¿ç”¨äº†SMOTEï¼Œä¿ç•™è¿™ä¸ªå‚æ•°ä¹Ÿå¯ä»¥è¿›ä¸€æ­¥å¢å¼ºæ¨¡å‹å¯¹å°‘æ•°ç±»çš„å…³æ³¨
rf_model = RandomForestClassifier(
    n_estimators=200,       # å¢åŠ æ ‘çš„æ•°é‡ä»¥æé«˜ç¨³å®šæ€§
    class_weight='balanced',
    random_state=42,
    max_depth=10,           # é™åˆ¶æ ‘çš„æ·±åº¦ä»¥é˜²è¿‡æ‹Ÿåˆ
    min_samples_leaf=5      # é™åˆ¶å¶èŠ‚ç‚¹æœ€å°‘æ ·æœ¬æ•°
)

print("å¼€å§‹åœ¨SMOTEå¢å¼ºåçš„è®­ç»ƒé›†ä¸Šè®­ç»ƒéšæœºæ£®æ—æ¨¡å‹...")
rf_model.fit(X_train_smote, y_train_smote)
print("æ¨¡å‹è®­ç»ƒå®Œæˆã€‚")

# åœ¨åŸå§‹æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹
print("\nåœ¨åŸå§‹æµ‹è¯•é›†ä¸Šè¿›è¡Œè¯„ä¼°...")
y_pred = rf_model.predict(X_test_scaled)

# å®šä¹‰ç›®æ ‡ç±»åˆ«åç§°
target_names = [label_names[i] for i in sorted(label_names.keys())]

# æ‰“å°åˆ†ç±»æŠ¥å‘Š
print("\nåˆ†ç±»æ€§èƒ½æŠ¥å‘Š:")
report = classification_report(y_test, y_pred, target_names=target_names)
print(report)

# æ‰“å°æ€»ä½“å‡†ç¡®ç‡
accuracy = accuracy_score(y_test, y_pred)
print(f"\næ¨¡å‹æ€»ä½“å‡†ç¡®ç‡: {accuracy:.4f}")
# è®¡ç®—æ··æ·†çŸ©é˜µ
cm = confusion_matrix(y_test, y_pred)
cm_df = pd.DataFrame(cm, index=target_names, columns=target_names)

# å¯è§†åŒ–æ··æ·†çŸ©é˜µ
plt.figure(figsize=(8, 6))
sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', cbar_kws={'label': 'æ ·æœ¬æ•°é‡'})
plt.title('éšæœºæ£®æ—äºŒåˆ†ç±»æ¨¡å‹æ··æ·†çŸ©é˜µ (SMOTEå¢å¼º)', fontproperties=font_prop, fontsize=16)
plt.ylabel('çœŸå®ç±»åˆ«', fontproperties=font_prop, fontsize=12)
plt.xlabel('é¢„æµ‹ç±»åˆ«', fontproperties=font_prop, fontsize=12)
plt.xticks(rotation=45)
plt.yticks(rotation=0)
plt.tight_layout()

# ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
os.makedirs('../../Paper/C_4', exist_ok=True)
plt.savefig('../../Paper/C_4/smote_rf_binary_confusion_matrix.png', dpi=300)
plt.show()

# ç‰¹å¾é‡è¦æ€§åˆ†æ
feature_importances = pd.DataFrame({
    'ç‰¹å¾': valid_features,
    'é‡è¦æ€§': rf_model.feature_importances_
}).sort_values('é‡è¦æ€§', ascending=False)

print("\næ¨¡å‹ç‰¹å¾é‡è¦æ€§ (Top 10):")
display(feature_importances.head(10))

# å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§
plt.figure(figsize=(10, 8))
sns.barplot(x='é‡è¦æ€§', y='ç‰¹å¾', data=feature_importances.head(15), palette='viridis')
plt.title('ç‰¹å¾é‡è¦æ€§ Top 15 (äºŒåˆ†ç±»æ¨¡å‹)', fontproperties=font_prop, fontsize=16)
plt.xlabel('é‡è¦æ€§', fontproperties=font_prop, fontsize=12)
plt.ylabel('ç‰¹å¾', fontproperties=font_prop, fontsize=12)
plt.tight_layout()
plt.savefig('../../Paper/C_4/smote_rf_binary_feature_importance.png', dpi=300)
plt.show()
# --- ç¬¬äºŒé˜¶æ®µï¼šä¸“å®¶è§„åˆ™å½’å›  ---

# 1. å®šä¹‰ä»ç¬¬ä¸€ä¸ªnotebookä¸­å­¦åˆ°çš„åŠ¨æ€é˜ˆå€¼
# è¿™äº›é˜ˆå€¼æ˜¯åœ¨å¹³è¡¡äº†ç²¾ç¡®ç‡å’Œå¬å›ç‡åå¾—åˆ°çš„
dynamic_thresholds = {
    'T13': 2.2, 
    'T18': 1.5, 
    'T21': 1.9
}
print("ä½¿ç”¨çš„åŠ¨æ€Zå€¼é˜ˆå€¼:")
print(dynamic_thresholds)

# 2. å®šä¹‰å½’å› å‡½æ•°
def attribute_abnormality(z13, z18, z21, thresholds):
    """æ ¹æ®Zå€¼å’ŒåŠ¨æ€é˜ˆå€¼è¿›è¡Œå¼‚å¸¸å½’å› """
    abnormalities = []
    if abs(z13) > thresholds['T13']:
        abnormalities.append('T13')
    if abs(z18) > thresholds['T18']:
        abnormalities.append('T18')
    if abs(z21) > thresholds['T21']:
        abnormalities.append('T21')
    
    if not abnormalities:
        # å¦‚æœAIæ¨¡å‹è®¤ä¸ºæ˜¯å¼‚å¸¸ï¼Œä½†Zå€¼è§„åˆ™æœªå‘ç°ä»»ä½•å¼‚å¸¸
        # è¿™å¯èƒ½æ˜¯æ¨¡å‹æ•æ‰åˆ°çš„ã€æ›´å¤æ‚çš„éZå€¼æ¨¡å¼ï¼Œæˆ–ä»…ä»…æ˜¯å‡é˜³æ€§
        # æˆ‘ä»¬å°†å…¶æ ‡è®°ä¸º "Abnormal_Unspecified" (æœªæ˜ç¡®çš„å¼‚å¸¸)
        return 'Abnormal_Unspecified'
    
    # æ ¹æ®T13, T18, T21çš„ç»„åˆæ¥ç¡®å®šæœ€ç»ˆæ ‡ç­¾
    # è¿™éƒ¨åˆ†é€»è¾‘éœ€è¦ä¸åŸå§‹æ•°æ®ä¸­çš„æ ‡ç­¾æ ¼å¼ä¿æŒä¸€è‡´
    if 'T13' in abnormalities and 'T18' in abnormalities:
        return 'T13T18'
    if 'T13' in abnormalities and 'T21' in abnormalities:
        return 'T13T21'
    if 'T18' in abnormalities and 'T21' in abnormalities:
        return 'T18T21'
    if 'T13' in abnormalities:
        return 'T13'
    if 'T18' in abnormalities:
        return 'T18'
    if 'T21' in abnormalities:
        return 'T21'
    
    return 'Abnormal_Unspecified' # å…œåº•

# --- åº”ç”¨å®Œæ•´çš„ä¸¤é˜¶æ®µæµç¨‹åˆ°æµ‹è¯•é›† ---

# ç¬¬ä¸€é˜¶æ®µçš„é¢„æµ‹ç»“æœ (0=Normal, 1=Any_Abnormal)
stage1_preds = y_pred

# è·å–æµ‹è¯•é›†çš„åŸå§‹æ•°æ®ï¼Œä»¥ä¾¿æå–Zå€¼
X_test_original = df.loc[X_test.index]

final_predictions = []
for i, prediction in enumerate(stage1_preds):
    if prediction == 0:  # ç¬¬ä¸€é˜¶æ®µé¢„æµ‹ä¸º 'Normal'
        final_predictions.append('Normal')
    else:  # ç¬¬ä¸€é˜¶æ®µé¢„æµ‹ä¸º 'Any_Abnormal'ï¼Œè¿›å…¥ç¬¬äºŒé˜¶æ®µ
        sample = X_test_original.iloc[i]
        z13 = sample['13å·æŸ“è‰²ä½“çš„Zå€¼']
        z18 = sample['18å·æŸ“è‰²ä½“çš„Zå€¼']
        z21 = sample['21å·æŸ“è‰²ä½“çš„Zå€¼']
        
        # åº”ç”¨è§„åˆ™è¿›è¡Œå½’å› 
        stage2_result = attribute_abnormality(z13, z18, z21, dynamic_thresholds)
        final_predictions.append(stage2_result)

# å°†åˆ—è¡¨è½¬æ¢ä¸ºSeriesï¼Œä¾¿äºåç»­åˆ†æ
final_predictions = pd.Series(final_predictions, index=X_test.index)

print("\nä¸¤é˜¶æ®µæµç¨‹å®Œæˆã€‚")
print("æœ€ç»ˆé¢„æµ‹ç»“æœåˆ†å¸ƒ:")
print(final_predictions.value_counts())
# è·å–æµ‹è¯•é›†çš„çœŸå®å¤šåˆ†ç±»æ ‡ç­¾
y_test_true_multiclass = df.loc[X_test.index, 'Detailed_Abnormality']

# æ•´åˆæ‰€æœ‰å‡ºç°è¿‡çš„ç±»åˆ«ï¼Œä»¥ç”Ÿæˆå®Œæ•´çš„æŠ¥å‘Š
all_labels = sorted(list(set(y_test_true_multiclass) | set(final_predictions)))

print("--- æœ€ç»ˆä¸¤é˜¶æ®µæ¨¡å‹æ€§èƒ½è¯„ä¼° ---")
print("\næœ€ç»ˆåˆ†ç±»æ€§èƒ½æŠ¥å‘Š:")
final_report = classification_report(
    y_test_true_multiclass, 
    final_predictions, 
    labels=all_labels,
    zero_division=0
)
print(final_report)

# è®¡ç®—å¹¶å¯è§†åŒ–æœ€ç»ˆçš„æ··æ·†çŸ©é˜µ
final_cm = confusion_matrix(y_test_true_multiclass, final_predictions, labels=all_labels)
final_cm_df = pd.DataFrame(final_cm, index=all_labels, columns=all_labels)

plt.figure(figsize=(12, 10))
sns.heatmap(final_cm_df, annot=True, fmt='d', cmap='Greens', cbar_kws={'label': 'æ ·æœ¬æ•°é‡'})
plt.title('æœ€ç»ˆä¸¤é˜¶æ®µæ¨¡å‹æ··æ·†çŸ©é˜µ', fontproperties=font_prop, fontsize=16)
plt.ylabel('çœŸå®ç±»åˆ«', fontproperties=font_prop, fontsize=12)
plt.xlabel('é¢„æµ‹ç±»åˆ«', fontproperties=font_prop, fontsize=12)
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.savefig('../../Paper/C_4/final_two_stage_confusion_matrix.png', dpi=300)
plt.show()
# åˆ›å»ºä¸€ä¸ªDataFrameæ¥å¯¹æ¯”çœŸå®æ ‡ç­¾å’Œæœ€ç»ˆé¢„æµ‹æ ‡ç­¾
comparison_df = pd.DataFrame({
    'çœŸå®ç±»åˆ«': y_test_true_multiclass,
    'é¢„æµ‹ç±»åˆ«': final_predictions
})

# è®¡ç®—äº¤å‰è¡¨ï¼Œç»Ÿè®¡æ¯ä¸ªçœŸå®ç±»åˆ«è¢«é¢„æµ‹ä¸ºå„ç§ç±»åˆ«çš„æ•°é‡
crosstab = pd.crosstab(comparison_df['çœŸå®ç±»åˆ«'], comparison_df['é¢„æµ‹ç±»åˆ«'])

# ç­›é€‰å‡ºæ‰€æœ‰å¼‚å¸¸ç±»åˆ«ï¼Œä¸åŒ…æ‹¬'Normal'ï¼Œå¹¶ä¸”åªé€‰æ‹©åœ¨æµ‹è¯•é›†çœŸå®æ ‡ç­¾ä¸­å®é™…å­˜åœ¨çš„ç±»åˆ«
abnormal_labels = [label for label in all_labels if label != 'Normal']
# åªé€‰æ‹©åœ¨crosstabè¡Œç´¢å¼•ä¸­å®é™…å­˜åœ¨çš„å¼‚å¸¸ç±»åˆ«
existing_abnormal_labels = [label for label in abnormal_labels if label in crosstab.index]

print(f"æ‰€æœ‰å¼‚å¸¸ç±»åˆ«: {abnormal_labels}")
print(f"æµ‹è¯•é›†ä¸­å®é™…å­˜åœ¨çš„å¼‚å¸¸ç±»åˆ«: {existing_abnormal_labels}")

if existing_abnormal_labels:
    crosstab_abnormal = crosstab.loc[existing_abnormal_labels]
    print("\nçœŸå®å¼‚å¸¸æ ·æœ¬çš„é¢„æµ‹åˆ†å¸ƒäº¤å‰è¡¨:")
    display(crosstab_abnormal)
else:
    print("\næµ‹è¯•é›†ä¸­æ²¡æœ‰å¼‚å¸¸æ ·æœ¬ï¼Œæ— æ³•ç”Ÿæˆå¼‚å¸¸æ ·æœ¬çš„é¢„æµ‹åˆ†å¸ƒäº¤å‰è¡¨ã€‚")
    crosstab_abnormal = pd.DataFrame()

# ç»˜åˆ¶å †å æ¡å½¢å›¾ï¼ˆåªåœ¨æœ‰æ•°æ®æ—¶ç»˜åˆ¶ï¼‰
if not crosstab_abnormal.empty:
    fig, ax = plt.subplots(figsize=(14, 8))
    crosstab_abnormal.plot(kind='bar', stacked=True, ax=ax, 
                           colormap='viridis', width=0.7)

    # æ·»åŠ æ ‡é¢˜å’Œæ ‡ç­¾
    ax.set_title('çœŸå®å¼‚å¸¸æ ·æœ¬çš„æœ€ç»ˆé¢„æµ‹åˆ†å¸ƒ', fontproperties=font_prop, fontsize=18, pad=20)
    ax.set_xlabel('çœŸå®å¼‚å¸¸ç±»åˆ«', fontproperties=font_prop, fontsize=14, labelpad=15)
    ax.set_ylabel('æ ·æœ¬æ•°é‡', fontproperties=font_prop, fontsize=14, labelpad=15)

    # ç¾åŒ–å›¾è¡¨
    ax.tick_params(axis='x', rotation=45, labelsize=12)
    ax.tick_params(axis='y', labelsize=12)
    ax.grid(axis='y', linestyle='--', alpha=0.7)
    ax.legend(title='é¢„æµ‹ç±»åˆ«', prop={'size': 12})

    # åœ¨æ¯ä¸ªå †å å—ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
    for c in ax.containers:
        labels = [int(v.get_height()) if v.get_height() > 0 else '' for v in c]
        ax.bar_label(c, labels=labels, label_type='center', color='white', fontsize=12, fontweight='bold')

    plt.tight_layout()
    plt.savefig('../../Paper/C_4/final_prediction_vs_true_distribution.png', dpi=300)
    plt.show()
else:
    print("ç”±äºæµ‹è¯•é›†ä¸­æ²¡æœ‰å¼‚å¸¸æ ·æœ¬ï¼Œè·³è¿‡å¼‚å¸¸æ ·æœ¬é¢„æµ‹åˆ†å¸ƒå›¾çš„ç»˜åˆ¶ã€‚")
    
y_train_true_multiclass = X_train_original_features['Detailed_Abnormality']

# åˆå§‹åŒ–è®­ç»ƒé›†çš„æœ€ç»ˆé¢„æµ‹Series
final_predictions_train = pd.Series(index=X_train.index, dtype=object)

# å¯¹è®­ç»ƒé›†ä¸­è¢«AIåˆ¤å®šä¸ºå¼‚å¸¸çš„æ ·æœ¬è¿›è¡Œå½’å› ï¼ˆä¿®å¤ç‰ˆï¼‰
abnormal_indices_train = X_train.index[y_train_pred_binary == 1]
print(f"   è®­ç»ƒé›†ä¸­è¢«AIåˆ¤å®šä¸ºå¼‚å¸¸çš„æ ·æœ¬æ•°: {len(abnormal_indices_train)}")

if len(abnormal_indices_train) > 0:
    # ä¿®å¤ï¼šä½¿ç”¨lambdaå‡½æ•°æ­£ç¡®ä¼ é€’å‚æ•°ç»™attribute_abnormalityå‡½æ•°
    attributed_results_train = X_train_original_features.loc[abnormal_indices_train].apply(
        lambda row: attribute_abnormality(
            row['13å·æŸ“è‰²ä½“çš„Zå€¼'], 
            row['18å·æŸ“è‰²ä½“çš„Zå€¼'], 
            row['21å·æŸ“è‰²ä½“çš„Zå€¼'], 
            dynamic_thresholds
        ),
        axis=1
    )
    final_predictions_train.loc[abnormal_indices_train] = attributed_results_train
    print(f"   è®­ç»ƒé›†å¼‚å¸¸æ ·æœ¬å½’å› å®Œæˆ: {len(attributed_results_train)}")
else:
    print("   è®­ç»ƒé›†ä¸­æ²¡æœ‰è¢«AIåˆ¤å®šä¸ºå¼‚å¸¸çš„æ ·æœ¬")

# å¡«å……è¢«AIåˆ¤å®šä¸ºæ­£å¸¸çš„æ ·æœ¬
normal_indices_train = X_train.index[y_train_pred_binary == 0]
final_predictions_train.loc[normal_indices_train] = 'Normal'
print(f"   è®­ç»ƒé›†æ­£å¸¸æ ·æœ¬æ•°: {len(normal_indices_train)}")
print("âœ… è®­ç»ƒé›†é¢„æµ‹å®Œæˆã€‚")

# 2. å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹
print("\nğŸ“Š å¤„ç†æµ‹è¯•é›†...")
X_test_for_pred_scaled = scaler.transform(X_test)
y_test_pred_binary = rf_model.predict(X_test_for_pred_scaled)

# è·å–æµ‹è¯•é›†çš„åŸå§‹ç‰¹å¾
X_test_original_features = df.loc[X_test.index]

# åˆå§‹åŒ–æµ‹è¯•é›†çš„æœ€ç»ˆé¢„æµ‹Series
final_predictions_test = pd.Series(index=X_test.index, dtype=object)

# å¯¹æµ‹è¯•é›†ä¸­è¢«AIåˆ¤å®šä¸ºå¼‚å¸¸çš„æ ·æœ¬è¿›è¡Œå½’å› ï¼ˆä¿®å¤ç‰ˆï¼‰
abnormal_indices_test = X_test.index[y_test_pred_binary == 1]
print(f"   æµ‹è¯•é›†ä¸­è¢«AIåˆ¤å®šä¸ºå¼‚å¸¸çš„æ ·æœ¬æ•°: {len(abnormal_indices_test)}")

if len(abnormal_indices_test) > 0:
    # ä¿®å¤ï¼šä½¿ç”¨lambdaå‡½æ•°æ­£ç¡®ä¼ é€’å‚æ•°ç»™attribute_abnormalityå‡½æ•°
    attributed_results_test = X_test_original_features.loc[abnormal_indices_test].apply(
        lambda row: attribute_abnormality(
            row['13å·æŸ“è‰²ä½“çš„Zå€¼'], 
            row['18å·æŸ“è‰²ä½“çš„Zå€¼'], 
            row['21å·æŸ“è‰²ä½“çš„Zå€¼'], 
            dynamic_thresholds
        ),
        axis=1
    )
    final_predictions_test.loc[abnormal_indices_test] = attributed_results_test
    print(f"   æµ‹è¯•é›†å¼‚å¸¸æ ·æœ¬å½’å› å®Œæˆ: {len(attributed_results_test)}")
else:
    print("   æµ‹è¯•é›†ä¸­æ²¡æœ‰è¢«AIåˆ¤å®šä¸ºå¼‚å¸¸çš„æ ·æœ¬")

# å¡«å……è¢«AIåˆ¤å®šä¸ºæ­£å¸¸çš„æ ·æœ¬
normal_indices_test = X_test.index[y_test_pred_binary == 0]
final_predictions_test.loc[normal_indices_test] = 'Normal'
print(f"   æµ‹è¯•é›†æ­£å¸¸æ ·æœ¬æ•°: {len(normal_indices_test)}")
print("âœ… æµ‹è¯•é›†é¢„æµ‹å®Œæˆã€‚")

# 3. åˆå¹¶æ‰€æœ‰é¢„æµ‹ç»“æœ
print("\nğŸ“Š åˆå¹¶é¢„æµ‹ç»“æœ...")
final_predictions_all = pd.concat([final_predictions_train, final_predictions_test])
print(f"æ€»é¢„æµ‹æ ·æœ¬æ•°: {len(final_predictions_all)}")

# 4. æ˜¾ç¤ºé¢„æµ‹ç»“æœç»Ÿè®¡
print("\nğŸ“ˆ é¢„æµ‹ç»“æœç»Ÿè®¡:")
prediction_counts = final_predictions_all.value_counts().sort_index()
for category, count in prediction_counts.items():
    percentage = count / len(final_predictions_all) * 100
    print(f"   {category}: {count} ({percentage:.2f}%)")

print("\nâœ… å…¨æ•°æ®é›†é¢„æµ‹å®Œæˆï¼")
print("="*70)

# --- å¯è§†åŒ–å…¨æ•°æ®é›†ä¸Šçš„æœ€ç»ˆé¢„æµ‹ç»“æœåˆ†å¸ƒ ---
print("æ­£åœ¨å¯è§†åŒ–æ¨¡å‹åœ¨å…¨æ•°æ®é›†ä¸Šçš„é¢„æµ‹ç»“æœåˆ†å¸ƒ...")

# 1. è®¡ç®—é¢„æµ‹ç»“æœçš„åˆ†å¸ƒ
prediction_counts = final_predictions_all.value_counts()

# 2. ç»˜åˆ¶æ¡å½¢å›¾
fig, ax = plt.subplots(figsize=(12, 7))
prediction_counts.plot(kind='bar', ax=ax, color='skyblue', width=0.7)

# æ·»åŠ æ ‡é¢˜å’Œæ ‡ç­¾
ax.set_title('æ¨¡å‹åœ¨å…¨æ•°æ®é›†ä¸Šçš„æœ€ç»ˆé¢„æµ‹ç»“æœåˆ†å¸ƒ', fontproperties=font_prop, fontsize=18, pad=20)
ax.set_xlabel('é¢„æµ‹ç±»åˆ«', fontproperties=font_prop, fontsize=14, labelpad=15)
ax.set_ylabel('æ ·æœ¬æ•°é‡', fontproperties=font_prop, fontsize=14, labelpad=15)

# ç¾åŒ–å›¾è¡¨
ax.tick_params(axis='x', rotation=45, labelsize=12)
ax.grid(axis='y', linestyle='--', alpha=0.6)

# åœ¨æ¡å½¢å›¾é¡¶éƒ¨æ·»åŠ æ•°å€¼æ ‡ç­¾
for i, count in enumerate(prediction_counts):
    ax.text(i, count + 3, str(count), ha='center', va='bottom', fontsize=12, fontweight='bold')

ax.set_ylim(0, prediction_counts.max() * 1.15) # è°ƒæ•´yè½´èŒƒå›´ä»¥å®¹çº³æ ‡ç­¾

plt.tight_layout()
plt.savefig('../../Paper/C_4/full_dataset_prediction_distribution.png', dpi=300)
plt.show()

print("å›¾è¡¨å·²ä¿å­˜è‡³ ../../Paper/C_4/full_dataset_prediction_distribution.png")

## 8. çœŸå®åˆ†å¸ƒ vs é¢„æµ‹åˆ†å¸ƒå¯¹æ¯”å¯è§†åŒ–

# è·å–å…¨æ•°æ®é›†çš„çœŸå®æ ‡ç­¾åˆ†å¸ƒ
true_distribution = df['Detailed_Abnormality'].value_counts()
pred_distribution = final_predictions_all.value_counts()

print("\n=== çœŸå®åˆ†å¸ƒ vs é¢„æµ‹åˆ†å¸ƒå¯¹æ¯” ===")
print("\nçœŸå®æ ‡ç­¾åˆ†å¸ƒ:")
print(true_distribution)
print("\né¢„æµ‹æ ‡ç­¾åˆ†å¸ƒ:")
print(pred_distribution)

# åˆå¹¶æ‰€æœ‰å¯èƒ½å‡ºç°çš„ç±»åˆ«
all_categories = sorted(list(set(true_distribution.index) | set(pred_distribution.index)))

# ç¡®ä¿ä¸¤ä¸ªåˆ†å¸ƒéƒ½åŒ…å«æ‰€æœ‰ç±»åˆ«ï¼ˆç”¨0å¡«å……ç¼ºå¤±çš„ç±»åˆ«ï¼‰
true_counts = [true_distribution.get(cat, 0) for cat in all_categories]
pred_counts = [pred_distribution.get(cat, 0) for cat in all_categories]

# åˆ›å»ºå¯¹æ¯”æ¡å½¢å›¾
x = np.arange(len(all_categories))
width = 0.35

fig, ax = plt.subplots(figsize=(15, 8))

# ç»˜åˆ¶ä¸¤ç»„æ¡å½¢å›¾
bars1 = ax.bar(x - width/2, true_counts, width, label='çœŸå®åˆ†å¸ƒ', color='lightcoral', alpha=0.8)
bars2 = ax.bar(x + width/2, pred_counts, width, label='é¢„æµ‹åˆ†å¸ƒ', color='skyblue', alpha=0.8)

# æ·»åŠ æ ‡é¢˜å’Œæ ‡ç­¾
ax.set_title('å…¨æ•°æ®é›†ï¼šçœŸå®åˆ†å¸ƒ vs æ¨¡å‹é¢„æµ‹åˆ†å¸ƒå¯¹æ¯”', fontproperties=font_prop, fontsize=18, pad=20)
ax.set_xlabel('ç±»åˆ«', fontproperties=font_prop, fontsize=14, labelpad=15)
ax.set_ylabel('æ ·æœ¬æ•°é‡', fontproperties=font_prop, fontsize=14, labelpad=15)
ax.set_xticks(x)
ax.set_xticklabels(all_categories, rotation=45, ha='right')
ax.legend(prop={'size': 12})

# æ·»åŠ ç½‘æ ¼
ax.grid(axis='y', linestyle='--', alpha=0.6)

# åœ¨æ¡å½¢å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
def add_value_labels(bars):
    for bar in bars:
        height = bar.get_height()
        if height > 0:  # åªåœ¨é«˜åº¦å¤§äº0æ—¶æ·»åŠ æ ‡ç­¾
            ax.annotate(f'{int(height)}',
                       xy=(bar.get_x() + bar.get_width() / 2, height),
                       xytext=(0, 3),  # 3 points vertical offset
                       textcoords="offset points",
                       ha='center', va='bottom',
                       fontsize=10, fontweight='bold')

add_value_labels(bars1)
add_value_labels(bars2)

# è°ƒæ•´yè½´èŒƒå›´
max_count = max(max(true_counts), max(pred_counts))
ax.set_ylim(0, max_count * 1.15)

plt.tight_layout()
plt.savefig('../../Paper/C_4/true_vs_predicted_distribution_comparison.png', dpi=300, bbox_inches='tight')
plt.show()

print("\nå¯¹æ¯”å›¾è¡¨å·²ä¿å­˜è‡³ ../../Paper/C_4/true_vs_predicted_distribution_comparison.png")

## 9. è¯¦ç»†æ€§èƒ½åˆ†æè¡¨æ ¼

# è®¡ç®—æ··æ·†çŸ©é˜µå¹¶ç”Ÿæˆè¯¦ç»†åˆ†æ
from sklearn.metrics import classification_report, confusion_matrix, cohen_kappa_score, matthews_corrcoef, balanced_accuracy_score
import pandas as pd

# ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
y_true_all = df['Detailed_Abnormality']
classification_rep = classification_report(y_true_all, final_predictions_all, output_dict=True, zero_division=0)

# è½¬æ¢ä¸ºDataFrameä»¥ä¾¿æ›´å¥½åœ°æ˜¾ç¤º
metrics_df = pd.DataFrame(classification_rep).transpose()
# åªä¿ç•™æˆ‘ä»¬å…³å¿ƒçš„æŒ‡æ ‡ï¼Œå¹¶é‡æ–°æ’åº
metrics_df = metrics_df[['precision', 'recall', 'f1-score', 'support']].round(4)

print("\n=== å„ç±»åˆ«è¯¦ç»†æ€§èƒ½æŒ‡æ ‡ ===")
display(metrics_df)

# è®¡ç®—æ··æ·†çŸ©é˜µ
all_categories = sorted(list(set(y_true_all) | set(final_predictions_all)))
cm_full = confusion_matrix(y_true_all, final_predictions_all, labels=all_categories)
cm_df = pd.DataFrame(cm_full, index=all_categories, columns=all_categories)

print("\n=== å®Œæ•´æ··æ·†çŸ©é˜µ ===")
display(cm_df)

# å¯è§†åŒ–æ··æ·†çŸ©é˜µ
import seaborn as sns
import matplotlib.pyplot as plt
plt.figure(figsize=(12, 10))
sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', cbar_kws={'label': 'æ ·æœ¬æ•°é‡'})
plt.title('å…¨æ•°æ®é›†æ··æ·†çŸ©é˜µ - çœŸå® vs é¢„æµ‹', fontproperties=font_prop, fontsize=16)
plt.ylabel('çœŸå®ç±»åˆ«', fontproperties=font_prop, fontsize=12)
plt.xlabel('é¢„æµ‹ç±»åˆ«', fontproperties=font_prop, fontsize=12)
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.savefig('../../Paper/C_4/full_dataset_confusion_matrix.png', dpi=300, bbox_inches='tight')
plt.show()

print("\næ··æ·†çŸ©é˜µå›¾è¡¨å·²ä¿å­˜è‡³ ../../Paper/C_4/full_dataset_confusion_matrix.png")

# --- å…³é”®æ€§èƒ½æŒ‡æ ‡æ±‡æ€» ---

# è®¡ç®—äºŒåˆ†ç±»æ€§èƒ½ï¼ˆNormal vs Any_Abnormalï¼‰
y_true_binary = (y_true_all != 'Normal').astype(int)
y_pred_binary = (final_predictions_all != 'Normal').astype(int)
from sklearn.metrics import precision_recall_fscore_support
binary_precision, binary_recall, binary_f1, _ = precision_recall_fscore_support(
    y_true_binary, y_pred_binary, average='binary'
)

# è®¡ç®—æ›´ä¸°å¯Œçš„è¯„ä¼°æŒ‡æ ‡
kappa = cohen_kappa_score(y_true_all, final_predictions_all)
balanced_acc = balanced_accuracy_score(y_true_all, final_predictions_all)
mcc = matthews_corrcoef(y_true_binary, y_pred_binary)


print(f"\n" + "="*15 + " æ¨¡å‹ç»¼åˆæ€§èƒ½è¯„ä¼° " + "="*15)

print(f"\n--- æ€»ä½“æ€§èƒ½ ---")
print(f"æ€»ä½“å‡†ç¡®ç‡ (Overall Accuracy): {classification_rep['accuracy']:.4f}")
print(f"å¹³è¡¡å‡†ç¡®ç‡ (Balanced Accuracy): {balanced_acc:.4f}")
print(f"å®å¹³å‡F1åˆ†æ•° (Macro Avg F1): {classification_rep['macro avg']['f1-score']:.4f}")
print(f"åŠ æƒå¹³å‡F1åˆ†æ•° (Weighted Avg F1): {classification_rep['weighted avg']['f1-score']:.4f}")
print(f"ç§‘æ©ç³»æ•° (Cohen's Kappa): {kappa:.4f}")

print(f"\n--- äºŒåˆ†ç±»æ€§èƒ½ (æ­£å¸¸ vs. å¼‚å¸¸) ---")
print(f"å¼‚å¸¸æ£€æµ‹ç²¾ç¡®ç‡ (Precision): {binary_precision:.4f}")
print(f"å¼‚å¸¸æ£€æµ‹å¬å›ç‡ (Recall/Sensitivity): {binary_recall:.4f}")
print(f"å¼‚å¸¸æ£€æµ‹F1åˆ†æ•° (F1-Score): {binary_f1:.4f}")
print(f"é©¬ä¿®æ–¯ç›¸å…³ç³»æ•° (MCC): {mcc:.4f}")


print("\n" + "="*17 + " æŒ‡æ ‡è§£è¯» " + "="*17)
print("å¹³è¡¡å‡†ç¡®ç‡ (Balanced Accuracy): åœ¨ä¸å¹³è¡¡æ•°æ®ä¸­æ¯”æ ‡å‡†å‡†ç¡®ç‡æ›´å…·å‚è€ƒä»·å€¼ï¼Œå®ƒå¯¹æ¯ä¸ªç±»åˆ«çš„å¬å›ç‡è¿›è¡Œå¹³å‡ã€‚")
print("ç§‘æ©ç³»æ•° (Cohen's Kappa): è¡¡é‡åˆ†ç±»ç»“æœä¸éšæœºåˆ†ç±»ç›¸æ¯”çš„æå‡ç¨‹åº¦ï¼Œå€¼åŸŸä¸º[-1, 1]ï¼Œ0è¡¨ç¤ºä¸éšæœºåˆ†ç±»æ— å¼‚ï¼Œ1è¡¨ç¤ºå®Œç¾åˆ†ç±»ã€‚")
print("é©¬ä¿®æ–¯ç›¸å…³ç³»æ•° (MCC): è¯„ä¼°äºŒåˆ†ç±»æ€§èƒ½çš„å‡è¡¡æŒ‡æ ‡ï¼Œç»¼åˆè€ƒè™‘äº†å››é¡¹æ··æ·†çŸ©é˜µå…ƒç´ ï¼Œå³ä½¿åœ¨ç±»åˆ«æä¸å¹³è¡¡æ—¶ä¹Ÿè¡¨ç°ç¨³å¥ï¼Œå€¼åŸŸä¸º[-1, 1]ï¼Œæ˜¯åæ˜ æ¨¡å‹æ•´ä½“æ€§çš„é‡è¦æŒ‡æ ‡ã€‚")